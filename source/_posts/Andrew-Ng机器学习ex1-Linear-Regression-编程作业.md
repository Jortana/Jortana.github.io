---
title: Andrew Ng 机器学习 ex1(Linear Regression) 编程作业
date: 2022-06-13 00:08:08
categories: 机器学习
tags:
    - 机器学习
    - Machine Learning
    - Coursera
    - 编程作业
    - Octave
    - Linear Regression
mathjax: true
---

在上周我开始了机器学习的学习，现在进行到第二周，开始涉及到编程作业，我自己在做的过程中也遇到了一些困难，有一些疑惑，所以想着记录一下，一来是加深自己的印象，二来可能帮助到其他的人。

学习资料和代码我均会上传到我的 Github，文章里我会尽量详述，如果需要查看整段代码请到 [这里](https://github.com/Jortana/coursera-machine-learning)自取。

<!-- more -->

不得不说，这门课程不仅老师讲得好，而且作业也非常清晰、好上手，有引导，有提示，难度梯度设计也很合理，我属于数学不是很好的，而且基本没学过线性代数，在听过讲解，并且看过课程提供的文字总结之后，也完全可以渐渐掌握方法，并顺利完成作业。

作业是写在一个 pdf 文件中的，文件名是 [ex1.pdf](https://github.com/Jortana/coursera-machine-learning/blob/main/ex1-octave/ex1.pdf)，点击可以查看。第一次作业除了热身之外，主要包含两个部分，分别是必做题和选做题。必做题是单个变量的线性回归（Linear regression with one variable），选做题是多个变量的线性回归（Linear regression with multiple variables）。这里还是不得不夸一下这份作业制作精良的程度，每一个文件的作用都清晰地标出来，并且在作业之前有一道非常简单并且提供答案的热身题让初学者熟悉环境以及运行和上传作业的操作，后面的必做题和选做题的难度设置让人感觉不至于太简单但是经过思考也能做出来，配合上上传和打分系统，正反馈十足。

## 1 热身练习 Simple Octave/MATLAB function

热身题和必做题的主文件是 `ex1.m`，文件中基本每一大块都有英文注释，建议同学们在做题之前可以先看一下当前部分是在做什么，代码为什么要这么写，对理解整个线性回归的过程是非常有帮助的。

热身题的要求就是创建并返回一个 $ 5 \times 5 $ 的单位矩阵（identity matrix），并且也给出了答案，只需要在 `warmUpExercise.m` 中合适的位置输入：

```Octave
A = eye(5);
```

## 2 单变量线性回归 Linear regression with one variable

这一部分是本周课程的重要内容，作业给了一个情景设定：假如你是一个加盟餐厅的 CEO，你要如何根据人口和利润这两个数据决定去哪个城市扩张你的规模。我们拿到的数据共有 97 条，每一条有 2 列，第 1 列是某地区的人口数，第 2 列是对应地区的利润。

要完成这个任务分为三个步骤，绘制数据，计算代价函数和用梯度下降法找最优值。

### 2.1 绘制数据 Plotting the Data

这一个环节我们需要完成的事情是把我们的数据用图表的形式呈现出来，有利于我们观察，也有利于我们后面找到方程之后检查是不是符合我们数据的走势。

需要完成的文件是 `plotData.m`，在作业中也给出了答案，在合适的位置输入：

```Ovtave
plot(x, y, 'rx', 'MarkerSize', 10);
ylabel('Profit in $10,000s');
xlabel('Pupolation of City in 01,000s');
```

### 2.2 计算代价函数 Computing the cost $ J(\theta) $

这个环节我们需要完成的文件是 `computeCost.m`，我们要完成 `function J = computeCost(X, y, theta)` 这个函数，函数要返回 $ J $ 的值，是一个具体的数，参数有三个，参数名就是字面的意思。

作业中给出了的代价方程计算公式：
$$
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2
$$

其中

$$
h_{\theta}(x) = \theta^{T}x = \theta_{0} + \theta_{1}x_{1}
$$

要注意的是，因为我们在计算 $ h_{\theta}(x) $ 时用了两个 $ \theta $，所以这里传过来的 `X` 是经过了一步处理的，原本的 $ X $ 是人口数，为了配合两个  $ \theta $，主程序在 `X` 的第一列加了一列的 `1`，也就是说将 **原本 $ m \times 1$ 的矩阵变成了 $ m \times 2 $ 的矩阵**。

在课程中老师也说过，这个式子最好用直接 **用矩阵或向量来计算**，而 **不是** 用循环计算。

$ h_{\theta}(x) $ 可以直接从公式中看出来，用 $ \theta $ 和 $ x $ 相乘即可，这里要注意，公式中的 $ \theta $ 是进行了转置操作的，在题目中并 **不需要** 进行这一步，因为主程序中生成 $ \theta $ 时，生成的就是一个 $ 2 \times 1 $ 的矩阵。$ y $ 直接就是一个列向量，所以 $ h_{\theta}(x^{(i)}) - y^{(i)}$ 可以用 `X * theta - y` 计算得出，计算得出的结果是一个 $ m \times 1 $ 的矩阵。

我们需要把上面计算的结果平方然后求和 `sum((X * theta - y) .^ 2)`，这里注意，在 Octave 中，**两个矩阵之间** 或者 **数和矩阵** 之间是可以直接进行 `*` 和 `-` 的操作的（在矩阵大小满足条件的前提下），但是**平方是不可以的**，所以这里要用 `.^`。

根据公式还需要乘以 $ \frac{1}{2m} $，所以最后我们需要补充在文件里的代码为：

```Octave
J = sum((X * theta - y) .^ 2) / (2 * m);
```

### 2.3 梯度下降 Gradient descent

这一步就是要用梯度下降法找到合适的 $ \theta $，完成我们的方程，要完成的文件是 `gradientDescent.m`，作业中也给出了公式：

$$
\theta_{j} := \theta_{j} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}
$$

上一步我们计算过 $ h_{\theta}(x^{(i)}) - y^{(i)} $ ，可以用 `X * theta - y` 计算得出，这一步要将这个结果乘上 $ x_{j}^{(i)} $ ，也就是乘上 $ X $ 矩阵中的每一个 $ x $，最后求和。换句话说，就是 $ X^{T} $ 乘 $ h_{\theta}(x^{(i)}) - y^{(i)} $，代码写成 `X' * (X * theta - y)` 。

最后，我们应该补充到文件中的代码应该是：

```Octave
theta = theta - alpha * X' * (X * theta - y) / m;
```

此时运行 `ex1.m` 就可以看到完整结果了，这个程序中还帮我们做了结果的可视化，可以清晰地看到 $ J(\theta) $ 与 $ \theta_{0} $ 和 $ \theta_{1} $ 的关系。

到这里必做题的部分就结束了，下面是选做题。

## 3 多变量线性回归 Linear regression with multiple variables

这一部分的任务是根据已有的数据预测房价，给我们的数据有 3 列，分别是房子的大小、卧室的数量和价格。这一部分的主程序文件是 `ex1_multi.m`。

这个任务需要我们用前面学过的两种不同的方法进行价格预测，分别是 **梯度下降法（Gradient descent）** 和 **正规方程法（Normal Equations）**。

### 3.1 梯度下降法 Gradient descent

在进行梯度下降前，我们观察数据，会发现数据中我们要用到的 **房子大小** 与 **卧室数量** 的大小差异比较大，房子大小是以平方英尺为单位，数据都是几千这个数量级，卧室数量最多也就只有 5，所以我们在进行计算之前先要做 **特征归一化**。

#### 3.1.1 特征归一化 Feature Normalization

对于每个特征，我们都需要做两件事：

* 减去这组特征的平均值

* 除以这组特征的标准差

在 Octave 里，我们可以用 `mean()` 和 `stad()` 两个函数来计算平均值和标准差，这两个函数都是支持输入向量和矩阵的，以 `mean(A)` 举例：

* 当 `A` 是向量时，返回 `A` 中元素的平均值，返回值是一个数

* 当 `A` 是矩阵时，将矩阵中的每一列看作一个向量，返回 `A` 中每一列元素的平均值，返回值是一个矩阵

在这个环节中，我们要补充的文件是 `featureNormalize.m`，函数的参数是 `X`，字面意思，这个函数有三个返回值：

* `X_norm` - 特征归一化处理后的 `X`

* `mu` - 每个特征的平均值

* `sigma` - 每个特征的标准差

`mu` 和 `sigma` 直接用函数计算可以得出，计算完成后对 `X` 进行计算即可：

```Octave
mu = mean(X);
sigma = std(X);
X_norm = (X_norm - mu) ./ sigma;
```

这里注意矩阵和矩阵之间是可以直接用 `-` 运算的，而我们要的结果是每一条数据和 `sigma` 中对应位置进行运算，所以这里要用 `./`。

这里提一句，在我们进行特征归一化处理返回 `X_norm` 之后，主程序是在这个矩阵的第一列加了一列的 `1` 的，以方便我们进行后续的计算。位于`ex1_multi.m` 55 行， `X = [ones(m, 1) X];`

#### 3.1.2 梯度下降 Gradient descent

特征归一化处理后我们就可以进行梯度下降计算了，由于我们在必做题部分写的梯度下降的算法没有用循环，是直接用矩阵进行计算的，是一个通用型的算法，所以可以直接拿过来。

在 `computeCostMulti.m` 文件中输入：

```Octave
J = sum((X * theta - y) .^ 2) / (2 * m);
```

在 `gradientDescentMulti.m` 文件中输入：

```Octave
theta = theta - alpha * X' * (X * theta - y) / m;
```

就完成了。

##### 更高效的代价函数计算方法

在这里，作业中也给出了在多变量时的另外一种计算代价函数的公式，用这个公式计算代价函数效率会更高：

$$
J(\theta)=\frac{1}{2m}(X\theta-\vec{y})^T(X\theta-\vec{y})
$$

实现这个公式，我们只需要把 `computeCostMulti.m` 中刚刚写的代码注释掉或者删掉，然后换成

```Octave
J = (X * theta - y)' * (X * theta - y) / (2 * m);
```

最后得出的结果也是正确的。

##### 更改学习率和迭代次数

作业中建议我们可以更改学习率和迭代次数来更高效地找到方程。

学习率和迭代次数都在 `ex1_multi.m` 定义了，分别是 85 行的 `alpha` 和 86 行的 `num_iters`，作业中建议我们将 `num_iters` 改到 `50`，我们可以只改这一个数据，然后运行`ex1_multi.m` 观察图像，然后调整 `alpha`，我改成 `0.1` 的时候觉得和作业上给出的参考图像差不多，这个属于自己主观的感受，大家可以随意调整，只要大致符合要求即可。

#### 3.1.3 价格预测

前面的准备工作做好之后，我们就可以来预测价格了，我们要预测的是一套 1650 平方英尺大并且有 3 个卧室的房子能卖多少钱。预测价格很简单，只需要将参数带入方程即可，需要注意，这里的参数也要做特征归一化处理，在 `ex1_multi.m` 中 107 行附近合适的位置输入代码：

```Octave
house_info = ([1650 3] - mu) ./ sigma;
house_info = [1, house_info];
price = house_info * theta;
```

### 3.2 正规方程法 Normal Equations

正规方程在层面就更加简单了，只需要计算方程得出 $ \theta $ 即可，也不需要进行特征归一化的处理。

#### 3.2.1 正规方程计算

计算的公式为：

$$
\theta = (X^{T}X)^{-1}X^{T}\vec{y}
$$

直接在 `normalEqn.m` 中输入：

```Octave
theta = pinv(X' * X) * X' * y;
```

#### 3.2.2 价格预测

用正规方程法得到的方程在进行预测时同样是不需要进行特征归一化的，在 `ex1_multi.m` 中 155 行附近合适的位置输入代码：

```Octave
house_info = [1 1650 3];
price = house_info * theta;
```

此时运行 `ex1_multi.m`，你应该会发现，两种方法预测出的价格相差不大，都是在 $293000 左右。

本次作业到此顺利完成。
